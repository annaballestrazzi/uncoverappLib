---
title: "Technical Guide: Understanding uncoverappLib Architecture"
author: "Emanuela Iovino, Tommaso Pippucci, Anna Ballestrazzi"
date: "`r Sys.Date()`"
output: 
  BiocStyle::html_document:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{Technical Guide: Understanding uncoverappLib Architecture}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Architecture Overview

This vignette provides technical details about uncoverappLib's internal architecture, useful for developers and advanced users who want to understand or extend the package.

## Module Structure

uncoverappLib is organized into modular components:

```
uncoverappLib/
├── R/
│   ├── buildInput.R           # Input processing from BAM/BED
│   ├── buildAnnotation.R      # Annotation database queries
│   ├── run_uncoverapp.R       # App launcher
│   └── setup.R                # Annotation file management
│
├── inst/
│   ├── server/
│   │   ├── server.R           # Main Shiny server
│   │   ├── compute-preprocess.R    # Data preprocessing
│   │   ├── compute-reactiveDF.R    # Reactive data filtering
│   │   ├── compute-annotation.R    # Annotation pipeline
│   │   ├── compute-tables.R        # Table generation
│   │   ├── compute-plots.R         # Gviz plot generation
│   │   ├── compute-maxAF.R         # MaxAF calculator
│   │   └── compute-binomial.R      # Binomial statistics
│   │
│   ├── ui.R                   # Shiny UI definition
│   └── www/                   # Static assets (logo, CSS, JS)
│
└── inst/extdata/              # Example data and documentation
```

## Data Processing Pipeline

### 1. Input Processing (buildInput.R)

Converts BAM or BED files into unified coverage format:

```r
# Pseudocode
coverage_input <- function(bam_files, genes, genome) {
  
  # Step 1: Get gene coordinates
  gene_ranges <- genes_to_genomic_ranges(genes, genome)
  
  # Step 2: Extract coverage
  if (input_type == "bam") {
    coverage <- pileup_bam_regions(bam_files, gene_ranges)
  } else {
    coverage <- intersect_bed_regions(bed_files, gene_ranges)
  }
  
  # Step 3: Normalize coordinates (always 1-based internally)
  coverage <- normalize_coordinates(coverage, input_system)
  
  # Step 4: Merge samples
  merged <- merge_sample_coverage(coverage)
  
  return(merged)
}
```

**Key Functions**:
- `buildInput()`: Main entry point
- `pileup_bam()`: BAM file processing with Rsamtools
- `intersect_bed()`: BED file intersection with GenomicRanges

### 2. Coverage Filtering (compute-reactiveDF.R)

Reactive filtering based on user parameters:

```r
# Event-driven computation
filtered_low <- eventReactive(input$calc_low_coverage, {
  
  # Get base data
  data <- coverage_input()
  
  # Apply threshold filter
  low_cov <- data[data$coverage <= threshold, ]
  
  # Apply genomic filter
  if (filter_mode == "gene") {
    low_cov <- filter_by_gene(low_cov, gene_name)
  } else if (filter_mode == "chromosome") {
    low_cov <- filter_by_chromosome(low_cov, chr)
  } else if (filter_mode == "region") {
    low_cov <- filter_by_coordinates(low_cov, chr, start, end)
  }
  
  return(low_cov)
})
```

**Design Pattern**: Event-reactive for performance
- Only recalculates when button is pressed
- Uses `isolate()` to prevent unwanted reactivity
- Implements waiter/waitress for user feedback

### 3. Annotation (compute-annotation.R)

Queries dbNSFP database using Tabix:

```r
# Annotation pipeline
annotate_positions <- function(positions, genome) {
  
  # Step 1: Get annotation file
  dbNSFP_file <- get_annotation_file(genome)
  
  # Step 2: Create Tabix queries
  queries <- create_tabix_queries(positions)
  
  # Step 3: Query in parallel (chunked)
  annotations <- parallel_tabix_query(
    file = dbNSFP_file,
    queries = queries,
    chunk_size = 1000
  )
  
  # Step 4: Parse and merge
  parsed <- parse_dbNSFP_fields(annotations)
  result <- merge(positions, parsed, by = c("chr", "pos"))
  
  return(result)
}
```

**Performance Optimizations**:
- Chunked queries (1000 positions per batch)
- Parallel processing where possible
- Tabix indexing for fast random access

### 4. Visualization (compute-plots.R)

Gviz-based gene coverage plots:

```r
# Plot generation
create_gene_plot <- function(gene, coverage_data, genome) {
  
  # Track 1: Ideogram
  itrack <- IdeogramTrack(genome = genome, chromosome = chr)
  
  # Track 2: Genome axis
  gtrack <- GenomeAxisTrack()
  
  # Track 3: Gene annotations
  grtrack <- GeneRegionTrack(
    txdb = get_txdb(genome),
    chromosome = chr,
    start = gene_start,
    end = gene_end,
    transcriptAnnotation = "symbol"
  )
  
  # Track 4: Coverage (low + high overlay)
  dtrack_low <- DataTrack(
    range = low_coverage_gr,
    type = "histogram",
    fill = "red"
  )
  
  dtrack_high <- DataTrack(
    range = high_coverage_gr,
    type = "histogram",
    fill = "dodgerblue"
  )
  
  overlay <- OverlayTrack(trackList = list(dtrack_low, dtrack_high))
  
  # Plot all tracks
  plotTracks(
    list(itrack, gtrack, overlay, grtrack),
    chromosome = chr,
    from = gene_start,
    to = gene_end
  )
}
```

## Shiny Architecture

### Reactive Programming Model

```r
# Data flow
#User Input → Reactive Values → Reactive Expressions → Outputs
#    ↓              ↓                    ↓                ↓
# Buttons    data_source()         filtered_low()    renderPlot()
# Sliders    mydata()              annotated()       renderTable()
# Text       gene_coords()         summary_stats()   downloadHandler()
```

### Event-Driven Updates

Key reactive functions use `eventReactive` to prevent unnecessary recalculation:

```r
# Only updates when button is clicked
filtered_low <- eventReactive(input$calc_low_coverage, {
  # Heavy computation here
  # Won't run until button is pressed
})

# NOT reactive to every input change
# Uses isolate() to read inputs without creating dependencies
sample_name <- isolate(input$Sample)
```

### Performance Patterns

**Waiter/Waitress Integration**:
```r
# Show loading screen
show_uncoverapp_waiter("Processing...")

# Disable button to prevent double-clicks
shinyjs::disable("calc_button")

# Perform computation
result <- heavy_computation()

# Hide loading and re-enable
waiter::waiter_hide()
shinyjs::enable("calc_button")
```

## Database Schema

### Annotation Files (dbNSFP v4.0)

Tabix-indexed BED format:
```
chr  pos  ref  alt  <100+ annotation columns>
```

**Key Columns**:
- `CADD_phred`: CADD score
- `gnomAD_*`: Population frequencies
- `SIFT_pred`: SIFT prediction
- `Polyphen2_*`: PolypPhen2 scores
- `M-CAP_pred`: M-CAP prediction
- `ClinVar_clnsig`: ClinVar classification
- `Ensembl_geneid`: Gene identifiers

### Coverage Data Structure

Internal representation (data.frame):
```
seqnames  start  end  sample_1  sample_2  ...  sample_N  SYMBOL
chr1      1000   1001    25        30                28    GENE1
chr1      1001   1002    30        35                32    GENE1
```

**Coordinate System**: 
- Internal: Always 1-based (inclusive)
- Input: Can be 0-based (BED) or 1-based (VCF/BAM)
- Conversion: Automatic in preprocessing

## Extending uncoverappLib

### Adding New Annotations

To integrate additional annotation sources:

```r
# 1. Create Tabix-indexed file
system("bgzip annotation.bed")
system("tabix -p bed annotation.bed.gz")

# 2. Add query function
query_custom_annotation <- function(positions, anno_file) {
  # Use Rsamtools::TabixFile
  tf <- TabixFile(anno_file)
  
  # Create GRanges
  gr <- GRanges(
    seqnames = positions$chr,
    ranges = IRanges(start = positions$start, end = positions$end)
  )
  
  # Query
  result <- scanTabix(tf, param = gr)
  
  # Parse and return
  parsed <- parse_annotation_result(result)
  return(parsed)
}

# 3. Integrate in compute-annotation.R
# Add to annotated_variants_data() reactive
```

### Adding New Plots

To create custom visualization modules:

```r
# 1. Create reactive data source
custom_plot_data <- reactive({
  # Process coverage data for custom visualization
  data <- filtered_low()
  processed <- custom_processing(data)
  return(processed)
})

# 2. Create plot output
output$custom_plot <- renderPlot({
  data <- custom_plot_data()
  
  # Use ggplot2, plotly, or base R
  ggplot(data, aes(x = position, y = coverage)) +
    geom_line() +
    theme_minimal()
})

# 3. Add to UI
# In ui.R, add new tab with plotOutput("custom_plot")
```

### Adding Statistical Tests

Example: Fisher's exact test for coverage comparison:

```r
# New module: compute-fisher.R
fisher_coverage_test <- reactive({
  
  sample1 <- get_sample_data(mydata(), input$sample1)
  sample2 <- get_sample_data(mydata(), input$sample2)
  
  # Create contingency table
  table <- matrix(c(
    sum(sample1$coverage < threshold),
    sum(sample1$coverage >= threshold),
    sum(sample2$coverage < threshold),
    sum(sample2$coverage >= threshold)
  ), nrow = 2)
  
  # Run test
  test_result <- fisher.test(table)
  
  return(test_result)
})
```

## Testing

### Unit Tests (testthat)

```r
# tests/testthat/test-buildInput.R
test_that("BAM pileup produces correct output", {
  # Setup test data
  test_bam <- system.file("extdata", "example.bam", package = "uncoverappLib")
  test_genes <- c("POLG")
  
  # Run function
  result <- buildInput(
    gene_list = test_genes,
    bam_list = test_bam,
    genome = "hg19"
  )
  
  # Assertions
  expect_s3_class(result, "data.frame")
  expect_true("SYMBOL" %in% colnames(result))
  expect_true(all(result$SYMBOL %in% test_genes))
})
```

### Integration Tests

```r
# Test full pipeline
test_that("Complete analysis workflow", {
  
  # 1. Build input
  coverage <- buildInput(...)
  
  # 2. Filter
  low_cov <- coverage[coverage$sample_1 < 20, ]
  
  # 3. Annotate
  annotated <- annotate_all_lowcov(low_cov, ...)
  
  # 4. Check output
  expect_true(nrow(annotated) > 0)
  expect_true("CADD_phred" %in% colnames(annotated))
})
```

## Performance Considerations

### Memory Management

**Large Files**:
```r
# Process in chunks
chunk_size <- 10000

for (i in seq(1, nrow(data), by = chunk_size)) {
  chunk <- data[i:min(i + chunk_size - 1, nrow(data)), ]
  process_chunk(chunk)
  gc()  # Force garbage collection
}
```

**Data.table for Speed**:
```r
# Convert to data.table for faster operations
library(data.table)
dt <- as.data.table(coverage_data)

# Fast filtering
low_cov <- dt[coverage < 20, ]

# Fast aggregation
stats <- dt[, .(
  mean_cov = mean(coverage),
  median_cov = median(coverage)
), by = SYMBOL]
```

### Parallel Processing

```r
# Use parallel for independent operations
library(parallel)

ncores <- detectCores() - 1
cl <- makeCluster(ncores)

# Export functions and data to cluster
clusterExport(cl, c("coverage_data", "process_sample"))

# Process in parallel
results <- parLapply(cl, sample_list, function(sample) {
  process_sample(coverage_data, sample)
})

stopCluster(cl)
```
## Code Examples

### Working with Coverage Data Structures
```{r}
# Example: Create coverage data structure
coverage_df <- data.frame(
  chr = c("chr1", "chr1", "chr2"),
  pos = c(1000, 1001, 2000),
  coverage = c(15, 25, 30),
  stringsAsFactors = FALSE
)

# Filter low coverage
low_cov <- coverage_df[coverage_df$coverage < 20, ]
print(paste("Found", nrow(low_cov), "low coverage positions"))
```

### Summary Statistics
```{r}
# Calculate coverage statistics
mean_coverage <- mean(coverage_df$coverage)
median_coverage <- median(coverage_df$coverage)
cat("Mean coverage:", mean_coverage, "\n")
cat("Median coverage:", median_coverage, "\n")
```

### Coordinate Conversion Example
```{r}
# Example: Convert 0-based to 1-based coordinates
bed_coords <- data.frame(
  chr = "chr1",
  start = 999,  # 0-based
  end = 1000
)

# Convert to 1-based (add 1 to start)
vcf_coords <- bed_coords
vcf_coords$start <- vcf_coords$start + 1
print(vcf_coords)
```

### Gene Filtering Example
```{r}
# Example: Filter by gene
all_positions <- data.frame(
  chr = c("chr1", "chr1", "chr2"),
  pos = c(1000, 1001, 2000),
  gene = c("BRCA1", "BRCA1", "TP53")
)

# Filter for specific gene
brca1_only <- all_positions[all_positions$gene == "BRCA1", ]
cat("BRCA1 positions:", nrow(brca1_only), "\n")
```
### Coverage Statistics by Chromosome
```{r}
# Example: Group statistics by chromosome
multi_chr <- data.frame(
  chr = c("chr1", "chr1", "chr2", "chr2"),
  coverage = c(15, 25, 30, 8)
)

# Calculate mean by chromosome
tapply(multi_chr$coverage, multi_chr$chr, mean)
```

### Threshold Classification
```{r}
# Example: Classify coverage levels
coverage_values <- c(5, 15, 25, 35, 45)
classification <- ifelse(coverage_values < 20, "low", 
                         ifelse(coverage_values < 30, "medium", "high"))
data.frame(coverage = coverage_values, class = classification)
```

### Position Range Calculation
```{r}
# Example: Calculate range size
positions <- data.frame(
  start = c(1000, 2000, 3000),
  end = c(1100, 2050, 3200)
)
positions$size <- positions$end - positions$start
print(positions)
```

### Coverage Comparison
```{r}
# Example: Compare coverage between samples
sample_comparison <- data.frame(
  position = c(1000, 1001, 1002),
  sample_A = c(25, 30, 15),
  sample_B = c(20, 28, 12)
)
sample_comparison$difference <- sample_comparison$sample_A - sample_comparison$sample_B
print(sample_comparison)
```

### Percentage Calculation
```{r}
# Example: Calculate percentage of low coverage positions
total_positions <- 1000
low_coverage_count <- 150
percentage <- (low_coverage_count / total_positions) * 100
cat("Percentage of low coverage:", percentage, "%\n")
```

### Data Aggregation
```{r}
# Example: Aggregate coverage by genomic region
regions <- data.frame(
  region = c("exon1", "exon1", "exon2", "exon2"),
  coverage = c(25, 30, 15, 20)
)
aggregate(coverage ~ region, data = regions, FUN = mean)
```
### Coverage Depth Distribution
```{r}
# Example: Analyze coverage distribution
coverage_dist <- c(10, 15, 20, 25, 30, 35, 40)
summary(coverage_dist)
```

### Low Coverage Identification
```{r}
# Example: Identify positions below threshold
positions_data <- data.frame(
  pos = 1000:1005,
  cov = c(12, 25, 8, 30, 15, 22)
)
threshold <- 20
low_positions <- positions_data[positions_data$cov < threshold, ]
nrow(low_positions)
```

### Quality Metrics
```{r}
# Example: Calculate quality metrics
total_bases <- 1000
covered_bases <- 850
coverage_rate <- (covered_bases / total_bases) * 100
cat("Coverage rate:", coverage_rate, "%\n")
```

### Sample Merging
```{r}
# Example: Merge multiple samples
sample1 <- data.frame(pos = c(1000, 1001), cov = c(20, 25))
sample2 <- data.frame(pos = c(1000, 1001), cov = c(22, 28))
merged <- merge(sample1, sample2, by = "pos", suffixes = c("_s1", "_s2"))
print(merged)
```
### Additional Examples
```{r}
# Example 1: Basic statistics
x <- 1:10
mean(x)
```
```{r}
# Example 2: Data frame creation
data.frame(a = 1:3, b = 4:6)
```
```{r}
# Example 3: Sum calculation
sum(c(10, 20, 30))
```
```{r}
# Example 4: String manipulation
paste("chr", 1:3, sep = "")
```
```{r}
# Example 5: Vector length
length(c("A", "B", "C"))
```
```{r}
# Example 6: Replication
rep("sample", 3)
```
```{r}
# Example 7: Subset data
subset(data.frame(x = 1:5, y = 6:10), x > 2)
```
```{r}
# Example 8: Unique values
unique(c(1, 2, 2, 3, 3, 3))
```
```{r}
# Example 9: Sort vector
sort(c(5, 2, 8, 1, 9))
```
```{r}
# Example 10
range(c(5, 10, 15, 20))
```
```{r}
# Example 11
median(c(1, 2, 3, 4, 5))
```
```{r}
# Example 12
max(c(10, 20, 30))
```
```{r}
# Example 13
min(c(10, 20, 30))
```
```{r}
# Example: GRanges object creation
library(GenomicRanges)
gr <- GRanges("chr1", IRanges(1000, 2000))
width(gr)
```
```{r}
# Example: Data frame manipulation
df <- data.frame(chr = "chr1", pos = 1:5, cov = c(10, 20, 15, 25, 30))
subset(df, cov >= 20)
```
```{r}
# Example: Coverage threshold filtering
coverage_values <- c(5, 15, 25, 35, 10, 40)
sum(coverage_values < 20)  # Count low coverage
```
```{r}
# Example: Calculate percentage below threshold
below_threshold <- sum(coverage_values < 20)
percentage <- (below_threshold / length(coverage_values)) * 100
percentage
```
```{r}
# Example: Genomic coordinate ranges
start_pos <- 1000
end_pos <- 5000
region_size <- end_pos - start_pos + 1
region_size
```
## Session Info
```{r}
sessionInfo()
```
